{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "def cleanData(csvLocation, name=\"default\"):\n",
    "    \n",
    "    def clean140(data):\n",
    "        data = pd.read_csv(data, encoding = \"latin1\")\n",
    "        data.columns=['sentiment', 'id', 'date', 'q', 'user', 'tweet']\n",
    "        data = data.drop(['id', 'date', 'q', 'user'], axis=1)\n",
    "        data = shuffle(data)\n",
    "        text = data.tweet\n",
    "        sentiment = data.sentiment\n",
    "        sentiment = pd.Series.tolist(sentiment)\n",
    "        for i in range(len(sentiment)):\n",
    "            j = sentiment[i]\n",
    "            if j==4:\n",
    "                sentiment[i]=1\n",
    "            elif j==2:\n",
    "                sentiment[i]=1\n",
    "\n",
    "        sentiment=np.asarray(sentiment)\n",
    "\n",
    "        trainData = []\n",
    "        for x in text:\n",
    "            trainData.append(''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x.lower())).split())\n",
    "        \n",
    "        return (trainData, sentiment)\n",
    "\n",
    "\n",
    "\n",
    "    if name == \"140\":\n",
    "        data, sentiment = clean140(csvLocation)\n",
    "        return (data, sentiment)\n",
    "        \n",
    "\n",
    "    data = pd.read_csv(csvLocation, sep='\\t')\n",
    "    data.columns = ['text', 'sentiment']\n",
    "    trainData=[]\n",
    "    for x in data.text:\n",
    "        trainData.append(''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x.lower())).split())\n",
    "        \n",
    "    return (trainData, data.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "maxlen = 50\n",
    "batch_size = 50\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abd/testEnv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, sentiment  = cleanData(\"sentiment140train.csv\", name= \"140\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('trainedW2V'):\n",
    "    wordModel=gensim.models.Word2Vec.load(\"trainedW2V\")\n",
    "else:\n",
    "    wordModel=gensim.models.Word2Vec(sentence, min_count=1, size=300, max_vocab_size=50000, iter=50, workers=50)\n",
    "    wordModel.save('trainedW2V')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abd/testEnv/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = wordModel.wv.syn0\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "\n",
    "def word2idx(word):\n",
    "    return wordModel.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return wordModel.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abd/testEnv/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = wordModel.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(sentence) :\n",
    "    preparedData = np.zeros([len(sentence), maxlen], dtype=np.int32)\n",
    "    for i, sentences in enumerate(sentence):\n",
    "      for t, word in enumerate(sentences[:-1]):\n",
    "        try:\n",
    "            preparedData[i, t] = word2idx(word)\n",
    "        except:\n",
    "            print(word + \" not found!\")\n",
    "            pass\n",
    "\n",
    "\n",
    "    print(preparedData.shape)\n",
    "    return preparedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th not found!\n",
      "the not found!\n",
      "other not found!\n",
      "(1599999, 50)\n"
     ]
    }
   ],
   "source": [
    "trainData = np.zeros([len(sentence), maxlen], dtype=np.int32)\n",
    "for i, sentences in enumerate(sentence):\n",
    "  for t, word in enumerate(sentences[:-1]):\n",
    "    try:\n",
    "        trainData[i, t] = word2idx(word)\n",
    "    except:\n",
    "        print(word + \" not found!\")\n",
    "        pass\n",
    "            \n",
    "\n",
    "print(trainData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.optimizers import Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 50)            250000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 48, 250)           37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 350,751\n",
      "Trainable params: 350,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1199999 samples, validate on 400000 samples\n",
      "Epoch 1/10\n",
      "1199999/1199999 [==============================] - 96s 80us/step - loss: 0.5249 - acc: 0.7331 - val_loss: 0.5245 - val_acc: 0.7330\n",
      "Epoch 2/10\n",
      "1199999/1199999 [==============================] - 92s 76us/step - loss: 0.5296 - acc: 0.7311 - val_loss: 0.5092 - val_acc: 0.7479\n",
      "Epoch 3/10\n",
      "1199999/1199999 [==============================] - 93s 78us/step - loss: 0.5182 - acc: 0.7415 - val_loss: 0.5007 - val_acc: 0.7541\n",
      "Epoch 4/10\n",
      "1199999/1199999 [==============================] - 95s 79us/step - loss: 0.5099 - acc: 0.7480 - val_loss: 0.4925 - val_acc: 0.7598\n",
      "Epoch 5/10\n",
      "1199999/1199999 [==============================] - 92s 77us/step - loss: 0.5050 - acc: 0.7520 - val_loss: 0.4859 - val_acc: 0.7646\n",
      "Epoch 6/10\n",
      "1199999/1199999 [==============================] - 92s 77us/step - loss: 0.4998 - acc: 0.7562 - val_loss: 0.4867 - val_acc: 0.7641\n",
      "Epoch 7/10\n",
      "1199999/1199999 [==============================] - 94s 79us/step - loss: 0.4969 - acc: 0.7583 - val_loss: 0.4815 - val_acc: 0.7677\n",
      "Epoch 8/10\n",
      "1199999/1199999 [==============================] - 95s 79us/step - loss: 0.4939 - acc: 0.7603 - val_loss: 0.4776 - val_acc: 0.7700\n",
      "Epoch 9/10\n",
      "1199999/1199999 [==============================] - 94s 78us/step - loss: 0.4922 - acc: 0.7616 - val_loss: 0.4773 - val_acc: 0.7698\n",
      "Epoch 10/10\n",
      "1199999/1199999 [==============================] - 97s 80us/step - loss: 0.4894 - acc: 0.7633 - val_loss: 0.4753 - val_acc: 0.7708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fafac30ca20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainData, sentiment, validation_split=0.25, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('project140.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSentiment(x):\n",
    "    x=[x]\n",
    "    a=[]\n",
    "    for i in x:\n",
    "        a.append(''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",i.lower())).split())\n",
    "    trainData = np.zeros([len(a), maxlen], dtype=np.int32)\n",
    "    for i, sentences in enumerate(a):\n",
    "      for t, word in enumerate(sentences[:]):\n",
    "        try:\n",
    "            trainData[i, t] = word2idx(word)\n",
    "        except:\n",
    "            print(word + \" not found!\")\n",
    "            pass\n",
    "    \n",
    "    score = model.predict(trainData)\n",
    "    print(score)\n",
    "    print(\"Positive with \", score*100, \"% probability.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59410244]]\n",
      "Positive with  [[59.410244]] % probability.\n"
     ]
    }
   ],
   "source": [
    "predictSentiment('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentiment140 testData</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, testSent = cleanData(\"sentiment140test.csv\", name= \"140\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497, 50)\n"
     ]
    }
   ],
   "source": [
    "testData=prepareData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = testSent-predictions.round().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7746478873239436"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors[errors==0])/len(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Yelp Movie reviews<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Keras version 2.1.6 detected. Last version known to be fully compatible of Keras is 2.1.3 .\n"
     ]
    }
   ],
   "source": [
    "import coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputLabels = ['Positive', 'Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : embedding_2_input, <keras.engine.topology.InputLayer object at 0x7fafa5e26c50>\n",
      "1 : embedding_2, <keras.layers.embeddings.Embedding object at 0x7fafa5e26c18>\n",
      "2 : embedding_2_permute_conv1d_2, <keras.layers.core.Permute object at 0x7fafa26cdc18>\n",
      "3 : conv1d_2, <keras.layers.convolutional.Conv1D object at 0x7fafa5e26d68>\n",
      "4 : conv1d_2__activation__, <keras.layers.core.Activation object at 0x7fafa3f08c88>\n",
      "5 : global_max_pooling1d_2, <keras.layers.pooling.GlobalMaxPooling1D object at 0x7fafa5e26e48>\n",
      "6 : global_max_pooling1d_2_permute_dense_3, <keras.layers.core.Permute object at 0x7fafa26b3eb8>\n",
      "7 : dense_3, <keras.layers.core.Dense object at 0x7fafa3fc8400>\n",
      "8 : activation_3, <keras.layers.core.Activation object at 0x7fafa3ff0080>\n",
      "9 : dense_4, <keras.layers.core.Dense object at 0x7fafa3ff0c18>\n",
      "10 : activation_4, <keras.layers.core.Activation object at 0x7fafa3fb0908>\n"
     ]
    }
   ],
   "source": [
    "coreMLSentiment = coremltools.converters.keras.convert('project140.h5', input_names=['sentence'], output_names=['sentiment'], class_labels=outputLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input {\n",
      "  name: \"sentence\"\n",
      "  type {\n",
      "    multiArrayType {\n",
      "      shape: 1\n",
      "      dataType: DOUBLE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"sentiment\"\n",
      "  type {\n",
      "    dictionaryType {\n",
      "      stringKeyType {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"classLabel\"\n",
      "  type {\n",
      "    stringType {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "predictedFeatureName: \"classLabel\"\n",
      "predictedProbabilitiesName: \"sentiment\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(coreMLSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreMLSentiment.author = 'iabd.me'\n",
    "coreMLSentiment.license = 'Dusk'\n",
    "coreMLSentiment.short_description = 'Sentiment prediction (Sentiment140)'\n",
    "coreMLSentiment.input_description['sentence'] = 'Input sentence'\n",
    "coreMLSentiment.output_description['sentiment'] = 'Probability of each sentiment'\n",
    "coreMLSentiment.output_description['classLabel'] = 'Labels of sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreMLSentiment.save('sentimentClassifier.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
