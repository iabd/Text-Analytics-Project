{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "def cleanData(csvLocation, name=\"default\"):\n",
    "    \n",
    "    def clean140(data):\n",
    "        data = pd.read_csv(data, encoding = \"latin1\")\n",
    "        data.columns=['sentiment', 'id', 'date', 'q', 'user', 'tweet']\n",
    "        data = data.drop(['id', 'date', 'q', 'user'], axis=1)\n",
    "        data = shuffle(data)\n",
    "        text = data.tweet\n",
    "        sentiment = data.sentiment\n",
    "        sentiment = pd.Series.tolist(sentiment)\n",
    "        for i in range(len(sentiment)):\n",
    "            j = sentiment[i]\n",
    "            if j==4:\n",
    "                sentiment[i]=1\n",
    "            elif j==2:\n",
    "                sentiment[i]=0.5\n",
    "\n",
    "        sentiment=np.asarray(sentiment)\n",
    "\n",
    "        trainData = []\n",
    "        for x in text:\n",
    "            trainData.append(''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x.lower())).split())\n",
    "        \n",
    "        return (trainData, sentiment)\n",
    "\n",
    "\n",
    "\n",
    "    if name == \"140\":\n",
    "        data, sentiment = clean140(csvLocation)\n",
    "        return (data, sentiment)\n",
    "        \n",
    "\n",
    "    data = pd.read_csv(csvLocation, sep='\\t')\n",
    "    data.columns = ['text', 'sentiment']\n",
    "    trainData=[]\n",
    "    for x in data.text:\n",
    "        trainData.append(''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x.lower())).split())\n",
    "        \n",
    "    return (trainData, data.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "maxlen = 50\n",
    "batch_size = 50\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abd/testEnv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, sentiment  = cleanData(\"sentiment140train.csv\", name= \"140\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('trainedW2V'):\n",
    "    wordModel=gensim.models.Word2Vec.load(\"trainedW2V\")\n",
    "else:\n",
    "    wordModel=gensim.models.Word2Vec(sentence, min_count=1, size=300, max_vocab_size=50000, iter=50, workers=50)\n",
    "    wordModel.save('trainedW2V')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abd/testEnv/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = wordModel.wv.syn0\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "\n",
    "def word2idx(word):\n",
    "    return wordModel.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return wordModel.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abd/testEnv/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = wordModel.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the not found!\n",
      "other not found!\n",
      "th not found!\n",
      "(1599999, 50)\n"
     ]
    }
   ],
   "source": [
    "trainData = np.zeros([len(sentence), maxlen], dtype=np.int32)\n",
    "for i, sentences in enumerate(sentence):\n",
    "  for t, word in enumerate(sentences[:-1]):\n",
    "    try:\n",
    "        trainData[i, t] = word2idx(word)\n",
    "    except:\n",
    "        print(word + \" not found!\")\n",
    "        pass\n",
    "            \n",
    "\n",
    "print(trainData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.optimizers import Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 50, 50)            250000    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 48, 250)           37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 350,751\n",
      "Trainable params: 350,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1279999 samples, validate on 320000 samples\n",
      "Epoch 1/10\n",
      "1279999/1279999 [==============================] - 147s 115us/step - loss: 0.5480 - acc: 0.7221 - val_loss: 0.5419 - val_acc: 0.7110\n",
      "Epoch 2/10\n",
      "1279999/1279999 [==============================] - 145s 113us/step - loss: 0.5344 - acc: 0.7223 - val_loss: 0.5255 - val_acc: 0.7289\n",
      "Epoch 3/10\n",
      "1279999/1279999 [==============================] - 145s 113us/step - loss: 0.5199 - acc: 0.7384 - val_loss: 0.5099 - val_acc: 0.7457\n",
      "Epoch 4/10\n",
      "1279999/1279999 [==============================] - 145s 113us/step - loss: 0.5089 - acc: 0.7478 - val_loss: 0.4977 - val_acc: 0.7549\n",
      "Epoch 5/10\n",
      "1279999/1279999 [==============================] - 146s 114us/step - loss: 0.5011 - acc: 0.7550 - val_loss: 0.4879 - val_acc: 0.7625\n",
      "Epoch 6/10\n",
      "1279999/1279999 [==============================] - 145s 113us/step - loss: 0.4960 - acc: 0.7584 - val_loss: 0.4845 - val_acc: 0.7653\n",
      "Epoch 7/10\n",
      "1279999/1279999 [==============================] - 145s 113us/step - loss: 0.4925 - acc: 0.7611 - val_loss: 0.4825 - val_acc: 0.7656\n",
      "Epoch 8/10\n",
      "1279999/1279999 [==============================] - 144s 113us/step - loss: 0.4896 - acc: 0.7630 - val_loss: 0.4817 - val_acc: 0.7682\n",
      "Epoch 9/10\n",
      "1279999/1279999 [==============================] - 148s 115us/step - loss: 0.4874 - acc: 0.7649 - val_loss: 0.4792 - val_acc: 0.7703\n",
      "Epoch 10/10\n",
      "1279999/1279999 [==============================] - 143s 112us/step - loss: 0.4853 - acc: 0.7662 - val_loss: 0.4757 - val_acc: 0.7711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c980ff908>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainData, sentiment, validation_split=0.20, epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
