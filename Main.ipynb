{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, gensim, sys, json\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, Sequential\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.optimizers import Adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "max_features = 5000\n",
    "maxlen = 50\n",
    "batch_size = 50\n",
    "embedding_dims = 50\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean140(data):\n",
    "        data = pd.read_csv(data, encoding = \"latin1\")\n",
    "        data.columns=['sentiment', 'id', 'date', 'q', 'user', 'tweet']\n",
    "        data = data.drop(['id', 'date', 'q', 'user'], axis=1)\n",
    "        data = shuffle(data)\n",
    "        text = data.tweet\n",
    "        sentiment = data.sentiment\n",
    "        sentiment = pd.Series.tolist(sentiment)\n",
    "        for i in range(len(sentiment)):\n",
    "            j = sentiment[i]\n",
    "            if j==4:\n",
    "                sentiment[i]=1\n",
    "            elif j==2:\n",
    "                sentiment[i]=1\n",
    "\n",
    "        sentiment=np.asarray(sentiment)\n",
    "\n",
    "        trainData = []\n",
    "        for x in text:\n",
    "            trainData.append(''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x.lower())).split())\n",
    "        \n",
    "        return (trainData, sentiment)\n",
    "def cleanData(csvLocation, name=\"default\"):\n",
    "    if name == \"140\":\n",
    "        data, sentiment = clean140(csvLocation)\n",
    "        return (data, sentiment)\n",
    "        \n",
    "\n",
    "    data = pd.read_csv(csvLocation, sep='\\t')\n",
    "    data.columns = ['text', 'sentiment']\n",
    "    trainData=[]\n",
    "    for x in data.text:\n",
    "        trainData.append(''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x.lower())).split())\n",
    "        \n",
    "    return (trainData, data.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Data\n",
    "sentence, sentiment  = cleanData(\"sentiment140train.csv\", name= \"140\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('trainedW2V'):\n",
    "    wordModel=gensim.models.Word2Vec.load(\"trainedW2V\")\n",
    "else:\n",
    "    wordModel=gensim.models.Word2Vec(sentence, min_count=1, size=300, max_vocab_size=50000, iter=50, workers=50)\n",
    "    wordModel.save('trainedW2V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abd/testEnv/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = wordModel.wv.syn0\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "\n",
    "def word2idx(word):\n",
    "    return wordModel.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return wordModel.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abd/testEnv/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = wordModel.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(sentence) :\n",
    "    \"\"\"\n",
    "    This function converts words to embedding metrices.\n",
    "    \"\"\"\n",
    "    preparedData = np.zeros([len(sentence), maxlen], dtype=np.int32)\n",
    "    for i, sentences in enumerate(sentence):\n",
    "      for t, word in enumerate(sentences[:-1]):\n",
    "        try:\n",
    "            preparedData[i, t] = word2idx(word)\n",
    "        except:\n",
    "            print(word + \" not found!\")\n",
    "            pass\n",
    "\n",
    "\n",
    "    print(preparedData.shape)\n",
    "    return preparedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = np.zeros([len(sentence), maxlen], dtype=np.int32)\n",
    "for i, sentences in enumerate(sentence):\n",
    "  for t, word in enumerate(sentences[:-1]):\n",
    "    try:\n",
    "        trainData[i, t] = word2idx(word)\n",
    "    except:\n",
    "        print(word + \" not found!\")\n",
    "        pass\n",
    "            \n",
    "\n",
    "print(trainData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            250000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 48, 250)           37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 350,751\n",
      "Trainable params: 350,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1199999 samples, validate on 400000 samples\n",
      "Epoch 1/10\n",
      "1199999/1199999 [==============================] - 96s 80us/step - loss: 0.5249 - acc: 0.7331 - val_loss: 0.5245 - val_acc: 0.7330\n",
      "Epoch 2/10\n",
      "1199999/1199999 [==============================] - 92s 76us/step - loss: 0.5296 - acc: 0.7311 - val_loss: 0.5092 - val_acc: 0.7479\n",
      "Epoch 3/10\n",
      "1199999/1199999 [==============================] - 93s 78us/step - loss: 0.5182 - acc: 0.7415 - val_loss: 0.5007 - val_acc: 0.7541\n",
      "Epoch 4/10\n",
      "1199999/1199999 [==============================] - 95s 79us/step - loss: 0.5099 - acc: 0.7480 - val_loss: 0.4925 - val_acc: 0.7598\n",
      "Epoch 5/10\n",
      "1199999/1199999 [==============================] - 92s 77us/step - loss: 0.5050 - acc: 0.7520 - val_loss: 0.4859 - val_acc: 0.7646\n",
      "Epoch 6/10\n",
      "1199999/1199999 [==============================] - 92s 77us/step - loss: 0.4998 - acc: 0.7562 - val_loss: 0.4867 - val_acc: 0.7641\n",
      "Epoch 7/10\n",
      "1199999/1199999 [==============================] - 94s 79us/step - loss: 0.4969 - acc: 0.7583 - val_loss: 0.4815 - val_acc: 0.7677\n",
      "Epoch 8/10\n",
      "1199999/1199999 [==============================] - 95s 79us/step - loss: 0.4939 - acc: 0.7603 - val_loss: 0.4776 - val_acc: 0.7700\n",
      "Epoch 9/10\n",
      "1199999/1199999 [==============================] - 94s 78us/step - loss: 0.4922 - acc: 0.7616 - val_loss: 0.4773 - val_acc: 0.7698\n",
      "Epoch 10/10\n",
      "1199999/1199999 [==============================] - 97s 80us/step - loss: 0.4894 - acc: 0.7633 - val_loss: 0.4753 - val_acc: 0.7708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fafac30ca20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainData, sentiment, validation_split=0.25, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# model.save('project140.h5')\n",
    "from keras.models import load_model\n",
    "model=load_model('project140.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSentiment(x):\n",
    "    x=[x]\n",
    "    a=[]\n",
    "    for i in x:\n",
    "        a.append(''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",i.lower())).split())\n",
    "    trainData = np.zeros([len(a), maxlen], dtype=np.int32)\n",
    "    for i, sentences in enumerate(a):\n",
    "      for t, word in enumerate(sentences[:]):\n",
    "        try:\n",
    "            trainData[i, t] = word2idx(word)\n",
    "        except:\n",
    "            print(word + \" not found!\")\n",
    "            pass\n",
    "    \n",
    "    score = model.predict(trainData)\n",
    "    print(score)\n",
    "    print(\"Positive with \", score*100, \"% probability.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58085936]]\n",
      "Positive with  [[58.085938]] % probability.\n"
     ]
    }
   ],
   "source": [
    "predictSentiment('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140 testData inference and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, testSent = cleanData(\"sentiment140test.csv\", name= \"140\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData=prepareData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = testSent-predictions.round().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6438631790744467"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors[errors==0])/len(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow version 1.13.1 detected. Last version known to be fully compatible is 1.12.0 .\n"
     ]
    }
   ],
   "source": [
    "import coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputLabels = ['Positive', 'Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreMLSentiment = coremltools.converters.keras.convert('project140.h5', input_names=['sentence'], output_names=['sentiment'], class_labels=outputLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input {\n",
      "  name: \"sentence\"\n",
      "  type {\n",
      "    multiArrayType {\n",
      "      shape: 1\n",
      "      dataType: DOUBLE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"sentiment\"\n",
      "  type {\n",
      "    dictionaryType {\n",
      "      stringKeyType {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"classLabel\"\n",
      "  type {\n",
      "    stringType {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "predictedFeatureName: \"classLabel\"\n",
      "predictedProbabilitiesName: \"sentiment\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(coreMLSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreMLSentiment.author = 'iabd.me'\n",
    "coreMLSentiment.license = 'Dusk'\n",
    "coreMLSentiment.short_description = 'Sentiment prediction (Sentiment140)'\n",
    "coreMLSentiment.input_description['sentence'] = 'Input sentence'\n",
    "coreMLSentiment.output_description['sentiment'] = 'Probability of each sentiment'\n",
    "coreMLSentiment.output_description['classLabel'] = 'Labels of sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreMLSentiment.save('sentimentClassifier.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
